{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "801a491b-5687-4107-8ea1-04e09d2ce930",
   "metadata": {},
   "source": [
    "## Tensor Operations:\n",
    "A tensor is a multi-dimensional array, similar to a NumPy array.\n",
    "In PyTorch, tensors can be created using the torch.Tensor class.\n",
    "Basic tensor operations like addition, subtraction, multiplication, and division can be performed using the standard arithmetic operators +, -, *, /.\n",
    "Here are some short exercises to help you get started:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333a049-c63b-4170-a466-0d796f65d33d",
   "metadata": {},
   "source": [
    "### Exercise 1: Creating Tensors\n",
    "Create a 2x3 tensor filled with zeros and a 3x2 tensor filled with ones using PyTorch. Print the tensors to verify that they are created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bd108a-890f-416d-b400-6710456d4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c826ade2-daa5-42c9-b829-f4cbde09dcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[7, 6, 0, 3, 0, 3],\n",
      "        [1, 0, 2, 3, 1, 6],\n",
      "        [6, 2, 0, 1, 1, 4],\n",
      "        [4, 6, 4, 5, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 2x3 tensor filled with zeros\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "print(zeros_tensor)\n",
    "\n",
    "# Create a 3x2 tensor filled with ones\n",
    "ones_tensor = torch.ones(3, 2)\n",
    "print(ones_tensor)\n",
    "\n",
    "rand_tensor = torch.randint(0, 9, (4,6))\n",
    "print(rand_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fd5d7-8b39-4789-8593-7eb159227de9",
   "metadata": {},
   "source": [
    "### Exercise 2: Performing Basic Tensor Operations\n",
    "Perform basic tensor operations using PyTorch. Create two tensors of the same shape and perform addition, subtraction, multiplication, and division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21fa3e31-d2c5-4dcd-ab81-58055cfeac0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "tensor([[-4, -4],\n",
      "        [-4, -4]])\n",
      "tensor([[ 5, 12],\n",
      "        [21, 32]])\n",
      "tensor([[0.2000, 0.3333],\n",
      "        [0.4286, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors of the same shape\n",
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Perform addition\n",
    "c = a + b\n",
    "print(c)\n",
    "\n",
    "# Perform subtraction\n",
    "d = a - b\n",
    "print(d)\n",
    "\n",
    "# Perform multiplication\n",
    "e = a * b\n",
    "print(e)\n",
    "\n",
    "# Perform division\n",
    "f = a / b\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc04b1-c7a9-4857-acb7-67a6bece11dc",
   "metadata": {},
   "source": [
    "## Automatic Differentiation:\n",
    "- Learn about automatic differentiation, a key feature of PyTorch that makes it easy to compute gradients for optimization\n",
    "- Understand the basics of backpropagation, which is used to train deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ecaf5-5951-4c76-b326-0247a67f98cc",
   "metadata": {},
   "source": [
    "## Building Neural Networks:\n",
    "- Learn how to build neural networks using PyTorch's nn module\n",
    "    - PyTorch's nn module makes it easy to construct and train neural networks. The basic idea is to define a neural network as a sequence of layers, where each layer performs a specific computation on the input data. PyTorch provides several types of layers, such as linear layers, convolutional layers, and recurrent layers, as well as various activation functions that can be used between the layers to introduce nonlinearity into the model.\n",
    "    - A simple example of building a neural network with PyTorch's nn module might involve defining a network architecture with a few layers (e.g., fully connected layers, convolutional layers, etc.) and specifying the activation functions to use between each layer. Here's a basic example:\n",
    "- Understand the different types of layers and activation functions used in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60095ff7-7a1d-4e04-b970-3cecfe94efb6",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e0ece4-2273-4f4b-8ddb-880f8fb44c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network with one hidden layer\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the neural network\n",
    "net = MyNet(input_size=10, hidden_size=20, output_size=1)\n",
    "\n",
    "# Apply the neural network to some input data\n",
    "x = torch.randn(32, 10)\n",
    "y = net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e739a8f-34cb-44dc-bf92-61c7f41bea28",
   "metadata": {},
   "source": [
    "In this example, we define a neural network with one hidden layer using PyTorch's nn module. The __init__ method initializes the layers of the network, and the forward method defines how the input data is transformed as it passes through the network. We also instantiate the neural network and apply it to some random input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00cf1d-6c9d-4512-b6d1-8e8b053762d8",
   "metadata": {},
   "source": [
    "Here are some exercises you can try to build your understanding of building neural networks in PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72a818-70ca-4aa6-88df-8b48a4b12721",
   "metadata": {},
   "source": [
    "1. Define a neural network with two hidden layers using the nn module.\n",
    "- Use the ReLU activation function for the hidden layers and the sigmoid activation function for the output layer.\n",
    "- Make the network input size 784 (for images in the MNIST dataset),\n",
    "    - the first hidden layer size 256\n",
    "    - the second hidden layer size 128\n",
    "    - the output layer size 10 (for the 10 possible digit classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4321ec36-1524-4ab0-9996-9d9aa3906f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network with two hidden layers\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "\n",
    "    #The forward method is called internally for the MyNet class by the __call__ method.\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the neural network\n",
    "net = MyNet()\n",
    "\n",
    "# Apply the neural network to some input data\n",
    "x = torch.randn(32, 784)\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3577530-42f8-4626-bd6b-85eafe7ad64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5004, 0.4929, 0.5017, 0.5127, 0.5248, 0.5417, 0.5030, 0.4986, 0.5419,\n",
       "         0.5190],\n",
       "        [0.5357, 0.4917, 0.4905, 0.4956, 0.4833, 0.5439, 0.5105, 0.5069, 0.4895,\n",
       "         0.4994],\n",
       "        [0.5166, 0.4670, 0.4735, 0.5098, 0.4692, 0.5226, 0.5181, 0.4940, 0.5525,\n",
       "         0.5072],\n",
       "        [0.5002, 0.4538, 0.5099, 0.4961, 0.4745, 0.5524, 0.4744, 0.4945, 0.5292,\n",
       "         0.4871],\n",
       "        [0.4998, 0.4816, 0.4829, 0.5142, 0.5030, 0.5116, 0.4734, 0.4780, 0.5164,\n",
       "         0.4857],\n",
       "        [0.4884, 0.4818, 0.4984, 0.4577, 0.4717, 0.5200, 0.5054, 0.5092, 0.5423,\n",
       "         0.5027],\n",
       "        [0.5088, 0.4642, 0.5112, 0.5008, 0.4825, 0.5251, 0.5374, 0.4762, 0.5271,\n",
       "         0.4702],\n",
       "        [0.4825, 0.4788, 0.4863, 0.5133, 0.4789, 0.5054, 0.5163, 0.4869, 0.5267,\n",
       "         0.4960],\n",
       "        [0.5197, 0.4767, 0.5050, 0.4841, 0.4627, 0.5038, 0.5099, 0.4691, 0.5331,\n",
       "         0.4681],\n",
       "        [0.4867, 0.4986, 0.5081, 0.5100, 0.4820, 0.5205, 0.5127, 0.5133, 0.5022,\n",
       "         0.5341],\n",
       "        [0.4906, 0.4564, 0.4861, 0.5088, 0.4683, 0.5399, 0.5133, 0.4697, 0.5472,\n",
       "         0.5250],\n",
       "        [0.5181, 0.4805, 0.5166, 0.5089, 0.5031, 0.5070, 0.5032, 0.4723, 0.5223,\n",
       "         0.4988],\n",
       "        [0.5343, 0.4852, 0.4965, 0.5310, 0.5052, 0.5185, 0.4492, 0.4921, 0.5238,\n",
       "         0.4850],\n",
       "        [0.5380, 0.4615, 0.5070, 0.5210, 0.4822, 0.5553, 0.4722, 0.4910, 0.5406,\n",
       "         0.4960],\n",
       "        [0.4982, 0.4965, 0.5019, 0.5017, 0.5044, 0.5392, 0.4892, 0.4468, 0.5300,\n",
       "         0.5006],\n",
       "        [0.4986, 0.4898, 0.5072, 0.5187, 0.4983, 0.5338, 0.4985, 0.4764, 0.5355,\n",
       "         0.5157],\n",
       "        [0.5186, 0.4745, 0.5106, 0.5363, 0.4624, 0.5154, 0.5036, 0.5025, 0.5425,\n",
       "         0.4972],\n",
       "        [0.5228, 0.4724, 0.4966, 0.5299, 0.5099, 0.5130, 0.4760, 0.4545, 0.5389,\n",
       "         0.4710],\n",
       "        [0.4923, 0.4707, 0.4952, 0.4973, 0.5228, 0.4977, 0.5044, 0.4475, 0.5197,\n",
       "         0.4882],\n",
       "        [0.5158, 0.4810, 0.4704, 0.4883, 0.4772, 0.5101, 0.5035, 0.4881, 0.5243,\n",
       "         0.4940],\n",
       "        [0.4850, 0.4708, 0.5395, 0.5058, 0.5101, 0.5154, 0.5301, 0.5065, 0.4940,\n",
       "         0.5011],\n",
       "        [0.5073, 0.4642, 0.5039, 0.5021, 0.4728, 0.5453, 0.4648, 0.4755, 0.4875,\n",
       "         0.4682],\n",
       "        [0.5093, 0.4834, 0.5096, 0.5084, 0.5068, 0.5200, 0.4720, 0.4955, 0.5339,\n",
       "         0.5059],\n",
       "        [0.5262, 0.4657, 0.4794, 0.5010, 0.4851, 0.5169, 0.5005, 0.4693, 0.5293,\n",
       "         0.4867],\n",
       "        [0.4977, 0.4970, 0.4971, 0.5286, 0.4807, 0.5334, 0.5438, 0.4886, 0.5207,\n",
       "         0.4653],\n",
       "        [0.5527, 0.4724, 0.5057, 0.5231, 0.4867, 0.5258, 0.4805, 0.4761, 0.5293,\n",
       "         0.4802],\n",
       "        [0.5374, 0.4600, 0.5160, 0.5258, 0.4743, 0.5426, 0.5198, 0.4809, 0.5168,\n",
       "         0.4811],\n",
       "        [0.4926, 0.4335, 0.4918, 0.4833, 0.4918, 0.5190, 0.5006, 0.4596, 0.5403,\n",
       "         0.5174],\n",
       "        [0.5177, 0.4953, 0.4868, 0.5388, 0.5082, 0.5387, 0.4639, 0.4963, 0.5459,\n",
       "         0.4911],\n",
       "        [0.4816, 0.5039, 0.5101, 0.5298, 0.4956, 0.5329, 0.4967, 0.4748, 0.4795,\n",
       "         0.5163],\n",
       "        [0.5063, 0.4936, 0.5242, 0.5095, 0.4936, 0.5344, 0.4984, 0.4927, 0.5295,\n",
       "         0.5108],\n",
       "        [0.5041, 0.4725, 0.4958, 0.5130, 0.5252, 0.5212, 0.5160, 0.5127, 0.5597,\n",
       "         0.4810]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1ec26-fc55-4df6-b886-d3c140c2da79",
   "metadata": {},
   "source": [
    "2. Define a neural network with a convolutional layer followed by two fully connected layers using the nn module. \n",
    "- Use the ReLU activation function for the hidden layers and the softmax activation function for the output layer. \n",
    "- Make the convolutional layer have 10 output channels, a kernel size of 5, and a stride of 1. \n",
    "- Make the first fully connected layer have 100 output units and the second fully connected layer have 10 output units (for the 10 possible digit classes in MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3eadc59-78c7-4c6e-8fcc-9028848059db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        \n",
    "    #The forward method is called internally for the MyNet class by the __call__ method.\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))\n",
    "        x = torch.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ec5b94-0df0-4906-a337-5679ee1970c1",
   "metadata": {},
   "source": [
    "## Training Neural Networks:\n",
    "- Learn how to train a neural network using PyTorch's autograd and optim modules\n",
    "- Understand the concepts of loss functions, optimization algorithms, and learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83da25-5846-4e9c-839f-8705c149a570",
   "metadata": {},
   "source": [
    "## Data Loading:\n",
    "- Learn how to load and preprocess data using PyTorch's DataLoader and transforms modules\n",
    "- Understand how to prepare data for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f078b9d-cbe5-4f44-b51b-c765281c5d42",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "- Learn how to evaluate the performance of a trained model on a test set\n",
    "- Understand the concepts of accuracy, precision, and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e20a9-860d-4fdf-9e33-b156d5dca09c",
   "metadata": {},
   "source": [
    "## Advanced Topics:\n",
    "- Learn about advanced topics:\n",
    "    - convolutional neural networks\n",
    "    - recurrent neural networks\n",
    "    - transfer learning\n",
    "- Understand how to use PyTorch for tasks:\n",
    "    - image classification\n",
    "    - natural language processing\n",
    "    - reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08e699-f187-41e3-a57b-f7570a8ea04e",
   "metadata": {},
   "source": [
    "## What are CNNs and what are they used for?\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a class of deep neural networks that are commonly used for image and video processing applications. They are particularly well-suited to problems where the input data has a grid-like structure, such as images, because they can learn spatial hierarchies of features from the data. CNNs consist of one or more convolutional layers, followed by one or more fully connected layers.\n",
    "\n",
    "## Convolutional layers and filters\n",
    "\n",
    "Convolutional layers are the building blocks of CNNs. They consist of a set of learnable filters, each of which is convolved with the input image to produce a set of feature maps. Each filter is a small matrix of weights that is trained to detect a particular feature in the input image, such as a horizontal or vertical edge. The output of a convolutional layer is a set of feature maps that represent different features of the input image.\n",
    "\n",
    "## Pooling layers\n",
    "\n",
    "Pooling layers are used to downsample the feature maps produced by the convolutional layers. They work by dividing the feature maps into non-overlapping regions and taking the maximum or average value within each region. This reduces the spatial dimensions of the feature maps while preserving the important features.\n",
    "\n",
    "## Activation functions for CNNs\n",
    "\n",
    "Activation functions are used to introduce nonlinearity into the output of the convolutional layers. This allows the CNN to learn more complex representations of the input data. Common activation functions for CNNs include ReLU (Rectified Linear Unit) and sigmoid.\n",
    "\n",
    "## Example: Image classification using a CNN\n",
    "An example of using CNNs is image classification, where the task is to classify an input image into one of several predefined categories. In this case, the CNN would consist of one or more convolutional layers, followed by one or more fully connected layers. The input image would be convolved with the learnable filters in the convolutional layers to produce a set of feature maps, which would then be downsampled using pooling layers. The output of the last fully connected layer would be passed through a softmax activation function to produce a probability distribution over the possible categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec569a7-25de-45db-b6b6-907b27a59d88",
   "metadata": {},
   "source": [
    "Project idea:\n",
    "\n",
    "Let's say you want to build a model to recognize emotions in speech data. \n",
    "- Use a CNN to extract features from the audio signal\n",
    "- Use an RNN to analyze the sequence of features over time. \n",
    "\n",
    "You could also use transfer learning by starting with a pre-trained CNN that was trained on a large dataset of audio, and fine-tuning it for your specific task of emotion recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc53d0c-6b1e-49ab-9146-62c2af16a78d",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a type of neural network that is specialized for sequence data, such as time series data or text. They are unique because they have recurrent layers, which allow them to maintain a \"memory\" of past inputs. This makes RNNs particularly useful for tasks that require context or long-term dependencies, such as predicting the next word in a sentence or the next value in a time series.\n",
    "\n",
    "RNNs operate by processing one input at a time and updating their internal memory state based on the current input and the previous memory state. This means that RNNs can take in sequences of varying lengths and output variable-length sequences as well.\n",
    "\n",
    "To improve the performance of RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) cells are often used in conjunction with them. These are specialized types of recurrent layers that allow RNNs to selectively retain or discard information in their memory, improving their ability to remember important information over long sequences.\n",
    "\n",
    "Overall, RNNs are a powerful tool for working with sequential data, and their ability to maintain memory across time makes them particularly useful for applications such as language modeling, speech recognition, and time series prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
